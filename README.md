<div align="center">
  <img src="./assets/sentinel-dac-logo.png" alt="SentinelDaC Logo" width="400">
  
  # SentinelDaC - Detection as Code Framework
  
  *A comprehensive, modular Detection as Code framework for Microsoft Sentinel*
</div>

---

## ğŸ“‘ Table of Contents

- [ğŸš€ Key Highlights](#-key-highlights)
- [ğŸ› ï¸ Getting Started](#%EF%B8%8F-getting-started)
  - [Prerequisites & Setup](#prerequisites--setup)
  - [Azure Infrastructure Setup](#azure-infrastructure-setup)
  - [Terraform Cloud Configuration](#terraform-cloud-configuration)
  - [Repository Configuration](#repository-configuration)
- [ğŸ—ï¸ Architecture](#%EF%B8%8F-architecture)
- [ğŸ¯ Framework Overview](#-framework-overview)
- [ğŸš€ Framework Status](#-framework-status)
  - [ğŸ“Š Current Deployment Stats](#-current-deployment-stats)
- [ğŸ”§ Configuration](#-configuration)
  - [Terraform Cloud Integration](#terraform-cloud-integration)
  - [Infrastructure Dependencies](#infrastructure-dependencies)
- [ğŸ“‹ Quick Start](#-quick-start)
- [ğŸ—ï¸ Component Workflow Diagrams](#%EF%B8%8F-component-workflow-diagrams)
  - [ğŸ”— Adding a New Data Connector](#-adding-a-new-data-connector)
  - [ğŸ“Š Adding a New Data Collection Rule (DCR)](#-adding-a-new-data-collection-rule-dcr)
  - [ğŸšª Adding a New Data Collection Endpoint (DCE)](#-adding-a-new-data-collection-endpoint-dce)
  - [ğŸ” Adding a Parser for Data Collection Rules](#-adding-a-parser-for-data-collection-rules)
- [ğŸ”„ Data Collection Rules (DCRs) & Logs Ingestion API](#-data-collection-rules-dcrs--logs-ingestion-api)
- [ğŸ›¡ï¸ Auto-Generated Detection Wiki](#%EF%B8%8F-auto-generated-detection-wiki)
  - [ğŸ¨ Live Wiki Screenshots](#-live-wiki-screenshots)
  - [ğŸ“‹ Watchlist Structure](#-watchlist-structure)
- [ğŸ”’ Security Considerations](#-security-considerations)
- [ğŸ“ˆ Environment Strategy](#-environment-strategy)

## ğŸš€ Key Highlights

- ğŸ—ï¸ **11 Core Components**: Detection rules (Scheduled, NRT, Microsoft), automation rules, watchlists, playbooks, custom tables, data collection (DCEs/DCRs), data connectors, threat intel rules
- ğŸ¯ **Framework Scope**: Focuses on Terraform-manageable components for programmatic control
- ğŸš€ **Modular Architecture**: Individual modules in `modules/` directories with comprehensive documentation
- ğŸ“‹ **Content Management**: Individual YAML files in `content/` directories for granular control
- âš™ï¸ **Single Configuration**: Simplified `terraform.tfvars` approach for easy management
- ğŸ“š **Auto-Generated Documentation**: Comprehensive Detection Wiki with MITRE ATT&CK mapping
- ğŸ”— **Data Collection Endpoints**: API ingestion capabilities similar to Splunk HEC
- ğŸ“Š **Remote State Management**: Terraform Cloud integration with cross-workspace dependencies

## ğŸ› ï¸ Getting Started

### Prerequisites & Setup

Before deploying the SentinelDaC framework, you'll need to complete the following setup steps:

#### **1. Azure Infrastructure Setup**

**Create Core Azure Resources:**
```bash
# 1. Create Azure Subscription (if needed)
# Use Azure Portal or contact your Azure administrator

# 2. Create Resource Group
az group create --name "rg-sentinel-yourenv" --location "Australia East"

# 3. Create Log Analytics Workspace
az monitor log-analytics workspace create --resource-group "rg-sentinel-yourenv" --workspace-name "law-sentinel-yourenv" --location "Australia East" --sku "PerGB2018"

# 4. Enable Microsoft Sentinel
az sentinel workspace create --resource-group "rg-sentinel-yourenv" --workspace-name "law-sentinel-yourenv"
```

#### **2. Service Principal for Terraform**

**Create App Registration & Service Principal:**
```bash
# 1. Create App Registration
az ad app create --display-name "terraform-sentineldac-yourenv" --query "appId" -o tsv
# Save the returned Application ID

# 2. Create Service Principal
az ad sp create --id "YOUR_APPLICATION_ID_FROM_STEP_1"

# 3. Generate Client Secret
az ad app credential reset --id "YOUR_APPLICATION_ID" --display-name "terraform-secret" --query "password" -o tsv
# Save the returned secret securely

# 4. Assign Contributor Permissions to Resource Group
az role assignment create --assignee "YOUR_APPLICATION_ID" --role "Contributor" --scope "/subscriptions/YOUR_SUBSCRIPTION_ID/resourceGroups/rg-sentinel-yourenv"

# 5. Assign Sentinel Contributor Permissions
az role assignment create --assignee "YOUR_APPLICATION_ID" --role "Microsoft Sentinel Contributor" --scope "/subscriptions/YOUR_SUBSCRIPTION_ID/resourceGroups/rg-sentinel-yourenv"
```

#### **3. Terraform Cloud Configuration**

**Set up Terraform Cloud (Free for up to 5 users):**

1. **Create Account**: Sign up at [https://app.terraform.io](https://app.terraform.io)
2. **Create Organization**: Choose a unique organization name
3. **Create Workspace**: 
   - Choose "Version control workflow"
   - Connect your forked GitHub repository
   - Name your workspace (e.g., `sentinel-content-yourenv`)

**Configure Environment Variables** (mark as Sensitive):
```
ARM_CLIENT_ID       = "your-service-principal-application-id"
ARM_CLIENT_SECRET   = "your-service-principal-secret"
ARM_SUBSCRIPTION_ID = "your-azure-subscription-id"
ARM_TENANT_ID       = "your-azure-tenant-id"
```

#### **4. Repository Configuration**

**Fork and Configure Repository:**
```bash
# 1. Fork this repository to your GitHub account

# 2. Clone your fork
git clone https://github.com/YOUR_USERNAME/SentinelxTerraformDaC.git
cd SentinelxTerraformDaC

# 3. Update terraform.tfvars with your values
# See SECURITY-SETUP.md for secure configuration guidance

# 4. Update main.tf with your Terraform Cloud organization
# Replace "Kaibersec" with your organization name
```

**Required terraform.tfvars Configuration:**
```hcl
# Infrastructure Settings - Update with your values
subscription_id              = "[YOUR_AZURE_SUBSCRIPTION_ID]"
resource_group_name          = "rg-sentinel-yourenv"
log_analytics_workspace_name = "law-sentinel-yourenv"

# Environment Configuration
environment = "dev"  # or "staging", "prod"

# Feature Toggles
enable_scheduled_rules     = true
enable_nrt_rules           = true
enable_microsoft_rules     = true
enable_playbooks           = true
enable_hunting_queries     = true
enable_automation_rules    = true
enable_watchlists          = true
enable_data_collection     = true
enable_data_connectors     = true
```

#### **5. GitHub Secrets Setup**

**For PowerShell Scripts & CI/CD** (See `SECURITY-SETUP.md` for details):
```
AZURE_TENANT_ID
AZURE_SUBSCRIPTION_ID  
AZURE_CLIENT_ID
AZURE_CLIENT_SECRET
```

#### **6. Initial Deployment**

```bash
# 1. Commit your configuration changes
git add terraform.tfvars
git commit -m "Initial configuration for my environment"
git push origin main

# 2. Terraform Cloud will automatically trigger a plan
# Review the plan in the Terraform Cloud UI

# 3. Approve the apply in Terraform Cloud UI
# This will deploy all 11 framework components

# 4. Verify deployment in Azure Portal
# Check Microsoft Sentinel > Analytics rules, Automation rules, etc.
```

#### **7. Pipeline setup**

You'll need to open up each of the pipelines and adjust the scheduling also if you want them to run as a cron job.

`.github\workflows\detection-wiki-generator.yml`
`.github\workflows\pull-request-sentinel-templates.yml`

This one is manually triggered though, once a pull request is accepted from the detection wiki generator. You would run this to push it to Confluence. Feel free to automate it though if the appetite is there.
`.github\workflows\push-wiki-to-confluence.yml`

#### **8. Post-Deployment Verification**

```powershell
# Test data ingestion (optional)
.\scripts\post-events-to-dce.ps1 -ClientSecret $env:AZURE_CLIENT_SECRET -ShowBody

# Generate detection wiki
.\scripts\generate-detection-wiki-clean.ps1

# Validate in Azure Portal:
# - Analytics Rules: Should see 10 detection rules
# - Automation Rules: Should see 1 active rule
# - Watchlists: Should see 3 lists with data
# - Custom Tables: Should see 11 custom tables
```

**ğŸ‰ Congratulations!** Your SentinelDaC framework is now deployed and ready for customization.

**Next Steps:**
- Review the [Quick Start](#-quick-start) section for common operations
- Explore [Component Workflow Diagrams](#%EF%B8%8F-component-workflow-diagrams) for adding new content
- Check the [Auto-Generated Detection Wiki](#%EF%B8%8F-auto-generated-detection-wiki) for documentation

## ğŸ—ï¸ Architecture

```
SentinelxTerraform-DaC/
â”œâ”€â”€ modules/                    # Reusable Terraform modules
â”‚   â”œâ”€â”€ analytics-rules/        # Scheduled analytics rules module âœ…
â”‚   â”œâ”€â”€ nrt-rules/             # Near Real Time rules module âœ…
â”‚   â”œâ”€â”€ microsoft-rules/       # Microsoft security rules module âœ…
â”‚   â”œâ”€â”€ automation-rules/      # Automation rules module âœ…
â”‚   â”œâ”€â”€ watchlists/           # Watchlists module âœ…
â”‚   â”œâ”€â”€ playbooks/            # Logic Apps (playbooks) module âœ…
â”‚   â”œâ”€â”€ hunting-queries/      # Hunting queries module âœ…
â”‚   â”œâ”€â”€ custom-tables/        # Custom Log Analytics tables module âœ…
â”‚   â”œâ”€â”€ data-collection/      # Data Collection Rules & Endpoints module âœ…
â”‚   â”œâ”€â”€ data-connectors/      # Data connectors module âœ…
â”‚   â””â”€â”€ threat-intel-rules/   # Threat intelligence rules module âœ…
â”œâ”€â”€ content/                   # Detection content definitions
â”‚   â”œâ”€â”€ scheduled-rules/       # 5 scheduled analytics rules (YAML)
â”‚   â”œâ”€â”€ nrt-rules/            # 1 near real-time rule (YAML)
â”‚   â”œâ”€â”€ microsoft-rules/       # 5 Microsoft security rules (YAML)
â”‚   â”œâ”€â”€ automation-rules/      # 1 automation rule + 2 disabled (YAML)
â”‚   â”œâ”€â”€ watchlists/           # 3 watchlists (metadata YAML + CSV data files)
â”‚   â”œâ”€â”€ playbooks/            # 3 active playbooks (YAML)
â”‚   â””â”€â”€ hunting-queries/      # 1 active hunting query (YAML)
â”œâ”€â”€ scripts/                   # PowerShell deployment & validation scripts
â”œâ”€â”€ DetectionWiki/            # Auto-generated detection documentation âœ…
â”‚   â”œâ”€â”€ DetectionWiki.md      # Comprehensive rule documentation with MITRE ATT&CK mapping
â”‚   â””â”€â”€ wiki-metadata.json   # Wiki generation metadata and statistics
â”œâ”€â”€ .github/workflows/        # CI/CD pipelines for automation âœ…
â”‚   â”œâ”€â”€ generate-detection-wiki-clean.ps1  # Wiki generation script
â”‚   â””â”€â”€ detection-wiki-generator.yml       # Automated wiki generation workflow
â”œâ”€â”€ *.tf                       # Terraform configuration files
â””â”€â”€ terraform.tfvars          # Single configuration file
```

## ğŸ¯ Framework Overview

This framework follows core principles of modularity, scalability, and centralised variable management to enable efficient deployment and management of Microsoft Sentinel detection content. The framework focuses on **11 core components** that are natively manageable through Terraform, providing programmatic control over detection and response capabilities.

### ğŸ¯ Framework Scope

**Terraform-Managed Components (Active):**
1. **Scheduled Rules** - Custom analytics rules with KQL queries (`analytics-rules` module)
2. **NRT Rules** - Near Real Time detection rules (`nrt-rules` module)
3. **Microsoft Rules** - Microsoft security product integration rules (`microsoft-rules` module)
4. **Automation Rules** - Incident assignment and response automation (`automation-rules` module)
5. **Watchlists** - Threat intelligence and reference data lists (`watchlists` module)
6. **Playbooks** - Logic Apps for automated response workflows (`playbooks` module)
7. **Hunting Queries** - Proactive threat hunting queries (`hunting-queries` module)
8. **Custom Tables** - Log Analytics custom tables for data ingestion (`custom-tables` module)
9. **Data Collection** - Data Collection Rules (DCRs) and Endpoints (DCEs) for log ingestion (`data-collection` module)
10. **Data Connectors** - Third-party and Microsoft service integrations (`data-connectors` module)
11. **Threat Intel Rules** - Threat intelligence correlation rules (`threat-intel-rules` module)

**Architectural Decision:**
The framework focuses on components that provide the best Terraform integration experience and are most suitable for programmatic management through Infrastructure as Code principles.

## ğŸš€ Framework Status

### ğŸ“Š What this environment builds
- **Detection Rules**: 10 active *(5 scheduled + 1 NRT + 5 Microsoft)*
- **Automation Rules**: 1 active *(2 disabled)*  
- **Watchlists**: 3 active with CSV data
- **Playbooks**: 3 active *(PowerShell, Python, Logic App)*
- **Hunting Queries**: 1 active
- **Custom Tables**: 11 active *(SecurityEvents_CL, MinimalLogs_CL + 8 ASIM schema tables)*
- **Data Collection Rules**: 11 active *(Linux syslog, Windows security, API security events + 8 ASIM schema DCRs)*
- **Data Collection Endpoints**: 2 active *(API ingestion, private ingestion)*
- **Data Connectors**: 1 active *(Microsoft Threat Intelligence)*
- **Threat Intel Rules**: 0 active *(module ready for deployment)*
- **ASIM Data Ingestion**: Complete 8-schema implementation with Custom-Asim* stream naming
- **Detection Wiki**: Auto-generated documentation with MITRE ATT&CK mapping âœ…
- **Framework Components**: 11/11 operational âœ…
- **Infrastructure**: Remote state management with cross-workspace dependencies

## ğŸ”§ Configuration

The framework uses a **single `terraform.tfvars` file** for simplified configuration management. The previous multi-environment directory structure has been removed in favor of a clean, single-configuration approach.

### Terraform Cloud Integration
- **Organisation**: Kaibersec
- **Workspace**: sentinel-content
- **Project**: Azure Infrastructure
- **Remote State**: Managed by Terraform Cloud
- **Authentication**: Service Principal (ARM_* environment variables)
- **Cross-Workspace Dependencies**: Consumes infrastructure state from `sentinellab-dev` workspace

### Infrastructure Dependencies
- **Infrastructure Workspace**: sentinellab-dev
- **Log Analytics Workspace**: law-sentinel-dev-2 (deployed via infrastructure workspace)
- **Sentinel Instance**: Enabled on Log Analytics workspace
- **Resource Group**: rg-sentinel-dev-2
- **Remote State Sharing**: Cross-workspace data source integration

## ğŸ“‹ Quick Start

> **ğŸ“š New to SentinelDaC?** Start with the comprehensive [Getting Started Guide](#%EF%B8%8F-getting-started) above for complete setup instructions.

**For existing deployments**, here are common operational tasks:

1. **Deploy framework changes**:
   ```bash
   # Review changes
   terraform plan

   # Apply changes  
   terraform apply
   ```

2. **Add a new detection rule**: Create a YAML file in the appropriate `content/` directory and redeploy

3. **Modify existing rules**: Edit the YAML files and re-run `terraform apply`
   ```bash
   terraform plan

   # Apply changes
   terraform apply

   # View deployment status
   terraform output
   
   ```

4. **Modify individual rules** by editing their YAML files and re-running `terraform apply`

5. **Test ASIM data ingestion** with the newly deployed DCRs:
   ```powershell
   # Test ASIM Network Session ingestion (validate only)
   .\scripts\post-events-to-dce-asim.ps1 -AsimSchema "NetworkSession" -InputJsonPath "Examples\Logs\network-session-sample.json" -ValidateOnly -ShowBody
   
   # Test ASIM Authentication events
   .\scripts\post-events-to-dce-asim.ps1 -AsimSchema "Authentication" -InputJsonPath "Examples\Logs\auth-sample.json" -ValidateOnly
   
   # Ingest real data to Custom-AsimNetworkSession stream
   .\scripts\post-events-to-dce-asim.ps1 -AsimSchema "NetworkSession" -InputJsonPath "Examples\Logs\network-session-sample.json" -ClientSecret "YOUR_SECRET"
   
   # Test all 8 ASIM schemas with retry protection
   .\scripts\test-all-asim-schemas.ps1 -ClientSecret "YOUR_SECRET" -ValidateOnly
   ```

6. **Query ASIM data** in Log Analytics:
   ```kusto
   // Query Custom-AsimNetworkSession events
   AsimNetworkSession_CL
   | where TimeGenerated > ago(24h)
   | summarize count() by EventVendor, EventProduct
   
   // Query Custom-AsimAuthentication events
   AsimAuthentication_CL 
   | where TimeGenerated > ago(24h)
   | where EventResult == "Failure"
   | project TimeGenerated, ActorUsername, SrcIpAddr, LogonMethod
   ```

## ğŸ—ï¸ ASIM Data Ingestion Infrastructure

### ğŸ“Š ASIM Schema Coverage
The framework includes complete Microsoft ASIM (Azure Sentinel Information Model) implementation with dedicated DCRs for optimal performance:

**Network & Communication:**
- ğŸŒ **Network Session** (`Custom-AsimNetworkSession`) - Network connection events
- ğŸ” **DNS Activity** (`Custom-AsimDnsActivity`) - DNS query and response events
- ğŸŒ **Web Session** (`Custom-AsimWebSession`) - HTTP/HTTPS web session events

**Authentication & Access:**
- ğŸ” **Authentication** (`Custom-AsimAuthentication`) - Login and authentication events

**System Activity:**
- âš™ï¸ **Process Events** (`Custom-AsimProcessEvent`) - Process creation and termination
- ğŸ“ **File Events** (`Custom-AsimFileEvent`) - File access and modification events
- ğŸ—‚ï¸ **Registry Events** (`Custom-AsimRegistryEvent`) - Windows registry operations

**Governance & Compliance:**
- ğŸ“‹ **Audit Events** (`Custom-AsimAuditEvent`) - System and user audit trails

### ğŸ¯ ASIM Architecture Benefits
- **Dedicated DCRs**: Each schema has its own DCR for maximum throughput (2GB/min per schema)
- **Custom Stream Naming**: Follows Azure Monitor requirements with `Custom-Asim*` prefix
- **Custom Table Integration**: Maps to dedicated custom tables with 90-day retention
- **API Ingestion Ready**: All DCRs use the API ingestion endpoint for external data sources
- **Transform Standardization**: Simple `source` transform for direct data ingestion
- **Scalable Design**: Independent schema processing prevents bottlenecks

## ğŸ—ï¸ Component Workflow Diagrams

> **These workflows show how to add new components after initial setup. For first-time setup, see [Getting Started](#%EF%B8%8F-getting-started).**

### ğŸ”— Adding a New Data Connector

```mermaid
graph LR
    A["ğŸ“<br/>Define<br/>Connector"] --> B["ğŸ”§<br/>Edit<br/>terraform.tfvars"]
    B --> C["ğŸ“‹<br/>Add to<br/>connector_configs"]
    C --> D["âš™ï¸<br/>Set Type<br/>& Settings"]
    D --> E["ğŸš€<br/>terraform<br/>apply"]
    E --> F["âœ…<br/>Connector<br/>Active"]
    
    classDef start fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef config fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef action fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    classDef deploy fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    
    class A start
    class B,C config
    class D action
    class E deploy
    class F success
```

### ğŸ“Š Adding a New Data Collection Rule (DCR)

```mermaid
graph LR
    A["ğŸ“<br/>Define<br/>DCR Config"] --> B["ğŸ”§<br/>Edit<br/>terraform.tfvars"]
    B --> C["ğŸ“‹<br/>Add to<br/>dcr_configs"]
    C --> D["âš™ï¸<br/>Set Data Type<br/>& Sources"]
    D --> E["ğŸ¯<br/>Configure<br/>Destinations"]
    E --> F["ğŸš€<br/>terraform<br/>apply"]
    F --> G["âœ…<br/>DCR<br/>Deployed"]
    
    classDef start fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef config fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef setup fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#000
    classDef action fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    classDef deploy fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    
    class A start
    class B,C config
    class D,E setup
    class F deploy
    class G success
```

### ğŸšª Adding a New Data Collection Endpoint (DCE)

```mermaid
graph LR
    A["ğŸ“<br/>Define<br/>DCE Config"] --> B["ğŸ”§<br/>Edit<br/>terraform.tfvars"]
    B --> C["ğŸ“‹<br/>Add to<br/>dce_configs"]
    C --> D["ğŸŒ<br/>Set Network<br/>Access"]
    D --> E["ğŸ“„<br/>Add<br/>Description"]
    E --> F["ğŸš€<br/>terraform<br/>apply"]
    F --> G["ğŸ”—<br/>API Endpoint<br/>Ready"]
    
    classDef start fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef config fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef setup fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#000
    classDef deploy fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    
    class A start
    class B,C config
    class D,E setup
    class F deploy
    class G success
```

###  Adding a New Custom Table

```mermaid
graph LR
    A["ğŸ“<br/>Design<br/>Table Schema"] --> B["ğŸ”§<br/>Edit<br/>terraform.tfvars"]
    B --> C["ğŸ“‹<br/>Add to<br/>custom_tables"]
    C --> D["âš™ï¸<br/>Define Columns<br/>& Types"]
    D --> E["ğŸ“…<br/>Set Retention<br/>Period"]
    E --> F["ğŸš€<br/>terraform<br/>apply"]
    F --> G["âœ…<br/>Custom Table<br/>Ready"]
    
    classDef start fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef config fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef setup fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#000
    classDef deploy fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    
    class A start
    class B,C config
    class D,E setup
    class F deploy
    class G success
```

### ğŸ” Adding a Parser for Data Collection Rules

```mermaid
graph LR
    A["ğŸ“<br/>Design<br/>KQL Parser"] --> B["ğŸ§ª<br/>Test in<br/>Sentinel"]
    B --> C["ğŸ“‹<br/>Define Stream<br/>Schema"]
    C --> D["ğŸ”§<br/>Update<br/>DCR Config"]
    D --> E["âš™ï¸<br/>Add Stream<br/>Declarations"]
    E --> F["ğŸ¯<br/>Configure<br/>Transformations"]
    F --> G["ğŸš€<br/>terraform<br/>apply"]
    G --> H["âœ…<br/>Parser<br/>Active"]
    
    classDef start fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef test fill:#f1f8e9,stroke:#689f38,stroke-width:2px,color:#000
    classDef design fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000
    classDef config fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef setup fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#000
    classDef deploy fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    
    class A start
    class B test
    class C design
    class D config
    class E,F setup
    class G deploy
    class H success
```

### ğŸ”„ **Complete Logs Ingestion Pipeline Setup (SOP)**

```mermaid
graph LR
    A["ğŸ¯<br/>Start:<br/>Design Log Schema"] --> B["ğŸ“‹<br/>1. Create Custom Table<br/>(modules/custom-tables)"]
    B --> C["ğŸ”—<br/>2. Create Data Collection<br/>Endpoint (DCE)"]
    C --> D["âš™ï¸<br/>3. Create Data Collection<br/>Rule (DCR)"]
    D --> E["ğŸ›¡ï¸<br/>4. Create App Registration<br/>& Permissions"]
    E --> F["ğŸ”§<br/>5. Update terraform.tfvars<br/>Configuration"]
    F --> G["ğŸš€<br/>6. Deploy Infrastructure<br/>(terraform apply)"]
    G --> H["ğŸ“<br/>7. Configure PowerShell<br/>Ingestion Script"]
    H --> I["ğŸ§ª<br/>8. Test with Sample<br/>JSON Data"]
    I --> J["âœ…<br/>9. Validate Data Flow<br/>in Log Analytics"]
    J --> K["ğŸ“Š<br/>10. Query Data<br/>in Sentinel"]
    
    classDef start fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000
    classDef phase1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef phase2 fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#000
    classDef phase3 fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#000
    
    class A start
    class B,C,D phase1
    class E,F,G phase2
    class H,I,J phase3
    class K success
```

**Standard Operating Procedure Steps:**

1. **ğŸ¯ Design Log Schema**: Define data structure, column types, and retention requirements
2. **ğŸ“‹ Create Custom Table**: Use azapi provider to create custom table with `_CL` suffix
3. **ğŸ”— Create DCE**: Deploy Data Collection Endpoint for HTTP ingestion
4. **âš™ï¸ Create DCR**: Configure Data Collection Rule with stream declarations
5. **ğŸ›¡ï¸ App Registration**: Create Azure AD app with "Monitoring Metrics Publisher" role
6. **ğŸ”§ Update Configuration**: Add all resources to terraform.tfvars
7. **ğŸš€ Deploy Infrastructure**: Execute terraform apply with dependency ordering
8. **ğŸ“ Configure Script**: Set up PowerShell with OAuth2 authentication
9. **ğŸ§ª Test Ingestion**: Validate with JSON sample data
10. **âœ… Validate Flow**: Confirm data appears in Log Analytics
11. **ğŸ“Š Query in Sentinel**: Verify data is available for detection rules

## ğŸ”„ Data Collection Rules (DCRs) & Logs Ingestion API

This section covers comprehensive learnings about Azure Monitor Data Collection Rules, custom tables, and the Logs Ingestion API - essential for building scalable data ingestion pipelines similar to Splunk HEC.

### ğŸ¯ **ASIM Schema DCR Architecture (NEW)**

Following Microsoft's best practices research, we've implemented **one DCR per ASIM schema** for optimal performance and scalability:

#### **8 ASIM Schema DCRs (Dedicated Performance Limits)**
- âœ… **asim_network_session**: `Custom-AsimNetworkSession` â†’ `Microsoft-ASimNetworkSessionLogs`
- âœ… **asim_dns_activity**: `Custom-AsimDnsActivity` â†’ `Microsoft-ASimDnsActivityLogs` 
- âœ… **asim_authentication**: `Custom-AsimAuthentication` â†’ `Microsoft-ASimAuthenticationEventLogs`
- âœ… **asim_process_event**: `Custom-AsimProcessEvent` â†’ `Microsoft-ASimProcessEventLogs`
- âœ… **asim_file_event**: `Custom-AsimFileEvent` â†’ `Microsoft-ASimFileEventLogs`
- âœ… **asim_registry_event**: `Custom-AsimRegistryEvent` â†’ `Microsoft-ASimRegistryEventLogs`
- âœ… **asim_web_session**: `Custom-AsimWebSession` â†’ `Microsoft-ASimWebSessionLogs`
- âœ… **asim_audit_event**: `Custom-AsimAuditEvent` â†’ `Microsoft-ASimAuditEventLogs`

#### **Performance Benefits**
- **ğŸš€ 8x Throughput**: Each DCR gets dedicated 2GB/min + 12K requests/min limits (16GB/min total)
- **ğŸ¯ Schema Isolation**: Independent monitoring, troubleshooting, and access control per schema
- **ğŸ“Š Microsoft Pattern**: Follows Azure Sentinel GitHub repository implementation patterns
- **âš¡ ASIM Script Ready**: Compatible with `post-events-to-dce-asim.ps1` auto-stream naming

#### **Stream Naming Convention**
```powershell
# Automatic stream resolution in ASIM script
$streamName = "Custom-Asim$schemaName"  # e.g., Custom-AsimNetworkSession
$outputStream = "Microsoft-ASim${schemaName}Logs"  # e.g., Microsoft-ASimNetworkSessionLogs
```

### ğŸ¯ **Core Concepts**

**Data Collection Rules (DCRs)** are Azure Monitor resources that define:
- **What data** to collect (stream declarations)
- **How to transform** it (KQL transformations)
- **Where to send** it (destinations)

**Custom Tables** are Log Analytics workspace tables with user-defined schemas that must exist **before** creating DCRs that target them.

**Logs Ingestion API** enables programmatic data submission to Azure Monitor via HTTP POST requests with OAuth2 authentication.

### ğŸ—ï¸ **Architecture Overview**

```mermaid
graph LR
    A["ğŸ“Š<br/>JSON Logs<br/>(Applications)"] --> B["ğŸ”—<br/>Data Collection<br/>Endpoint (DCE)"]
    B --> C["âš™ï¸<br/>Data Collection<br/>Rule (DCR)"]
    C --> D["ğŸ”„<br/>KQL<br/>Transformation"]
    D --> E["ğŸ“‹<br/>Custom Table<br/>(Log Analytics)"]
    E --> F["ğŸ›¡ï¸<br/>Microsoft<br/>Sentinel"]
    
    classDef source fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#000
    classDef endpoint fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef processing fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#000
    classDef storage fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    classDef security fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000
    
    class A source
    class B endpoint
    class C,D processing
    class E storage
    class F security
```

### ğŸ“‹ **Custom Tables: Foundation First**

Custom tables **must be created before** DCRs. They define the schema and retention for your data.

#### **Terraform Implementation**
```terraform
# Custom Tables Module (modules/custom-tables/)
resource "azapi_resource" "custom_tables" {
  for_each = var.custom_tables

  type      = "Microsoft.OperationalInsights/workspaces/tables@2022-10-01"
  name      = each.value.name
  parent_id = "/subscriptions/${data.azurerm_client_config.current.subscription_id}/resourceGroups/${var.resource_group_name}/providers/Microsoft.OperationalInsights/workspaces/${var.workspace_name}"

  body = jsonencode({
    properties = {
      schema = {
        name        = each.value.name
        description = each.value.description
        columns     = each.value.columns
      }
      retentionInDays = each.value.retention_days
      plan           = "Analytics"
    }
  })
}
```

#### **Configuration Example**
```terraform
custom_tables = {
  security_events = {
    name = "SecurityEvents_CL"
    description = "Custom table for security events ingested via DCR API"
    columns = [
      { name = "TimeGenerated", type = "datetime" },
      { name = "EventType", type = "string" },
      { name = "SourceIP", type = "string" },
      { name = "DestinationIP", type = "string" },
      { name = "Protocol", type = "string" },
      { name = "Action", type = "string" },
      { name = "Severity", type = "string" },
      { name = "Message", type = "string" }
    ]
    retention_days = 90
  }
}
```

#### **Key Requirements**
- âœ… **Naming Convention**: Must end with `_CL` (Custom Log)
- âœ… **TimeGenerated Column**: Required for all custom tables
- âœ… **Column Types**: `datetime`, `string`, `int`, `long`, `real`, `bool`, `dynamic`
- âœ… **azapi Provider**: Required as AzureRM provider doesn't support custom tables
- âš ï¸ **Create First**: Tables must exist before DCRs can reference them

### ğŸ”— **Data Collection Endpoints (DCEs)**

DCEs provide HTTP endpoints for data ingestion, similar to Splunk's HTTP Event Collector (HEC).

#### **Types of DCEs**
1. **Public API Endpoint**: External applications, public internet access
2. **Private Endpoint**: Internal systems, VNet integration, enhanced security

#### **Configuration Example**
```terraform
dce_configs = {
  api_ingestion = {
    name                          = "dce-api-ingestion-dev"
    description                   = "API endpoint for custom application data ingestion"
    public_network_access_enabled = true
  }
  
  private_ingestion = {
    name                          = "dce-private-ingestion-dev"
    description                   = "Private endpoint for internal system data ingestion"
    public_network_access_enabled = false
  }
}
```

#### **Generated Endpoints**
- **API Ingestion**: `https://dce-api-ingestion-dev-1tty.australiaeast-1.ingest.monitor.azure.com`
- **Private Ingestion**: `https://dce-private-ingestion-dev-am22.australiaeast-1.ingest.monitor.azure.com`

### âš™ï¸ **Data Collection Rules (DCRs)**

DCRs define the data processing pipeline from endpoint to destination.

#### **DCR Components**
1. **Stream Declarations**: Define input data schema
2. **Data Flows**: Map streams to destinations with transformations
3. **Destinations**: Target Log Analytics workspaces
4. **Transformations**: KQL queries to modify data

#### **Complete DCR Configuration**
```terraform
api_logs_security = {
  name         = "dcr-api-security-events-dev"
  data_type    = "custom"
  destinations = ["primary"]
  data_collection_endpoint_key = "api_ingestion"
  
  # Stream declarations - define input schema
  stream_declarations = {
    "Custom-SecurityEvents" = {
      columns = [
        { name = "TimeGenerated", type = "datetime" },
        { name = "EventType", type = "string" },
        { name = "SourceIP", type = "string" },
        { name = "DestinationIP", type = "string" },
        { name = "Protocol", type = "string" },
        { name = "Action", type = "string" },
        { name = "Severity", type = "string" },
        { name = "Message", type = "string" }
      ]
    }
  }
  
  # Output stream - target custom table
  output_stream = "Custom-SecurityEvents_CL"
  
  # KQL transformation
  transform_kql = "source | extend TimeGenerated = now()"
}
```

#### **Critical DCR Requirements**
- âœ… **Stream Schema Match**: Stream columns must match custom table schema exactly
- âœ… **Unique Stream Names**: Each DCR needs unique stream names even for same table
- âœ… **Output Stream Format**: Must match custom table name (e.g., `Custom-SecurityEvents_CL`)
- âœ… **DCE Reference**: Must reference an existing Data Collection Endpoint
- âš ï¸ **Table Dependency**: Custom table must exist before DCR creation

### ğŸ” **Authentication & Permissions**

#### **Required Azure AD App Registration**
```powershell
# 1. Create App Registration
az ad app create --display-name "DCR-API-Ingestion-App" --query "appId" -o tsv

# 2. Create Service Principal
az ad sp create --id "APP_ID_FROM_STEP_1"

# 3. Generate Client Secret
az ad app credential reset --id "APP_ID" --display-name "DCR-Ingestion-Secret" --query "password" -o tsv

# 4. Assign Required Role
az role assignment create --assignee "APP_ID" --role "Monitoring Metrics Publisher" --scope "RESOURCE_GROUP_SCOPE"
```

#### **OAuth2 Authentication Flow**
```powershell
# PowerShell Authentication Example
$tokenEndpoint = "https://login.microsoftonline.com/$TenantId/oauth2/v2.0/token"
$body = @{
    grant_type    = "client_credentials"
    client_id     = $ClientId
    client_secret = $ClientSecret
    scope         = "https://monitor.azure.com/.default"  # Critical: monitor.azure.com scope
}
$resp = Invoke-RestMethod -Method Post -Uri $tokenEndpoint -Body $body -ContentType "application/x-www-form-urlencoded"
$token = $resp.access_token
```

### ğŸ“Š **Logs Ingestion API Usage**

#### **API Endpoint Structure**
```
POST https://{DCE_ENDPOINT}/dataCollectionRules/{DCR_IMMUTABLE_ID}/streams/{STREAM_NAME}?api-version=2023-01-01

Headers:
- Authorization: Bearer {OAuth2_Token}
- Content-Type: application/json
- Content-Encoding: gzip (optional)

Body: JSON array of events
```

#### **PowerShell Implementation**
```powershell
# Complete PowerShell script for data ingestion
param(
    [string]$DceBaseUrl = "https://your-dce-url-here.australiaeast-1.ingest.monitor.azure.com",
    [string]$DcrImmutableId = "your-immutable-id-here",
    [string]$StreamName = "Custom-SecurityEvents",
    [string]$InputJsonPath
)

# Authentication
$token = Get-OAuthToken -TenantId $TenantId -ClientId $ClientId -ClientSecret $ClientSecret

# Build URI
$uri = "$DceBaseUrl/dataCollectionRules/$DcrImmutableId/streams/$StreamName" + "?api-version=2023-01-01"

# Prepare headers
$headers = @{ 
    Authorization = "Bearer $token"
    'Content-Type' = 'application/json'
}

# Send data
$response = Invoke-WebRequest -Method Post -Uri $uri -Headers $headers -Body $jsonPayload -UseBasicParsing
```

### ğŸ”„ **Data Processing Flow**

#### **JSON Field Mapping**
The framework automatically maps common JSON fields to custom table schema:

```powershell
# Example: security-events.json â†’ SecurityEvents_CL mapping
$processedEvents += @{ 
    "TimeGenerated" = if ($eventItem.timestamp) { $eventItem.timestamp } else { (Get-Date).ToString("yyyy-MM-ddTHH:mm:ss.fffZ") }
    "EventType" = if ($eventItem.source) { $eventItem.source } else { "SecurityEvent" }
    "SourceIP" = if ($eventItem.sourceIP) { $eventItem.sourceIP } else { "" }
    "DestinationIP" = if ($eventItem.destinationIP) { $eventItem.destinationIP } else { "" }
    "Protocol" = if ($eventItem.protocol) { $eventItem.protocol.ToUpper() } else { "" }
    "Action" = if ($eventItem.action) { $eventItem.action } else { "" }
    "Severity" = if ($eventItem.severity) { $eventItem.severity } else { "Medium" }
    "Message" = if ($eventItem.message) { $eventItem.message } else { ($eventItem | ConvertTo-Json -Compress) }
}
```

#### **KQL Transformations**
DCRs support KQL transformations to enrich or modify data:

```kql
// Basic transformation - add ingestion timestamp
source | extend TimeGenerated = now()

// Advanced transformation - enrich with metadata
source 
| extend TimeGenerated = now()
| extend DataSource = 'API_Ingestion'
| extend ProcessedBy = 'DCR_SecurityEvents'
| where isnotempty(SourceIP)
```

### ğŸ¯ **Multiple DCRs, Same Table**

You can have multiple DCRs writing to the same custom table with different stream names:

```terraform
# DCR 1: API ingestion
api_logs_security = {
    stream_declarations = {
        "Custom-SecurityEvents" = { ... }  # Stream name 1
    }
    output_stream = "Custom-SecurityEvents_CL"  # Same table
}

# DCR 2: File ingestion  
file_logs_security = {
    stream_declarations = {
        "Custom-SecurityEventsFile" = { ... }  # Different stream name
    }
    output_stream = "Custom-SecurityEvents_CL"  # Same table
}
```

**Requirements for Same Table:**
- âœ… **Identical Schema**: All stream declarations must have exact same columns
- âœ… **Unique Stream Names**: Each DCR needs different stream names
- âœ… **Compatible Transformations**: KQL transformations should produce same output schema

### ğŸš¨ **Common Issues & Solutions**

#### **1. ASIM DCR Architecture**
```terraform
# âœ… Correct: One DCR per ASIM schema (Microsoft best practice)
asim_network_session = {
  stream_declarations = {
    "Custom-AsimNetworkSession" = { columns = [...] }
  }
  output_stream = "Microsoft-ASimNetworkSessionLogs"
}

asim_dns_activity = {
  stream_declarations = {
    "Custom-AsimDnsActivity" = { columns = [...] }
  }
  output_stream = "Microsoft-ASimDnsActivityLogs"
}
# ... 6 more ASIM DCRs for complete coverage
```

#### **2. Provider Source Issues**
```terraform
# Fix: Correct azapi provider source
terraform {
  required_providers {
    azapi = {
      source  = "azure/azapi"  # Not "hashicorp/azapi"
      version = "~> 1.0"
    }
  }
}
```

#### **3. Custom Table Creation Order**
```terraform
# Fix: Explicit dependency ordering
module "custom_tables" {
  source = "./modules/custom-tables"
  # ... configuration
}

module "data_collection" {
  source = "./modules/data-collection"
  depends_on = [module.custom_tables]  # Ensure tables exist first
  # ... configuration
}
```

#### **4. Authentication Scope**
```powershell
# Fix: Correct OAuth2 scope
scope = "https://monitor.azure.com/.default"  # Not .azure.com
```

#### **5. JSON Array Format**
```powershell
# Fix: Ensure JSON array format
$bodyJson = ($processedEvents | ConvertTo-Json -Depth 64 -AsArray)  # -AsArray is critical
```

### ğŸ“ˆ **Performance & Best Practices**

#### **Batch Size Optimization**
- âœ… **Recommended**: 1MB - 25MB per request
- âœ… **Max Events**: 1000 events per batch
- âœ… **Compression**: Use gzip for payloads > 1KB

#### **Error Handling & Data Loss Prevention**

**Critical Rate Limiting Behavior:**
- âœ… **HTTP 204**: Success (No Content)
- âš ï¸ **HTTP 429**: Rate limiting - **RECOVERABLE** (temporary throttling)
- âŒ **HTTP 400**: Invalid data format
- âŒ **HTTP 401**: Authentication failure  
- âŒ **HTTP 403**: Permission denied
- âŒ **HTTP 413**: Payload too large

**ğŸš¨ Data Loss Risk - HTTP 429 Handling:**

When DCR limits are exceeded (2GB/min or 12K requests/min per DCR), Azure Monitor returns HTTP 429 with `Retry-After` header. **Without proper retry logic, data is permanently lost.**

```powershell
# âŒ BAD: Data loss occurs on rate limiting
try {
    Invoke-WebRequest -Uri $uri -Method Post -Body $data
    Write-Host "Success"
} catch {
    Write-Error "Failed - DATA LOST!"  # 429 errors lose data forever
}

# âœ… GOOD: Data preserved with retry logic  
$result = Send-DataWithRetry -Uri $uri -Body $data -MaxRetries 5
if (-not $result.Success) {
    # Data preserved for manual retry or queue persistence
    Save-DataForRetry -Data $data -Reason $result.Error
}
```

**Enhanced ASIM Script Features:**
- âœ… **Automatic Retry**: Up to 5 attempts with exponential backoff
- âœ… **Retry-After Compliance**: Honors Azure's specified wait times  
- âœ… **Data Preservation**: No data loss on temporary rate limiting
- âœ… **Error Classification**: Distinguishes recoverable vs permanent failures
- âœ… **Detailed Logging**: Retry attempts, wait times, and failure reasons

**DCR Rate Limiting Recovery:**
```bash
# When rate limited, Azure responds with:
HTTP 429 Too Many Requests
Retry-After: 60

# Script automatically waits 60 seconds and retries
# Data is preserved throughout retry process
```

#### **Monitoring & Alerting**
```kql
// Monitor ingestion success rates
SecurityEvents_CL
| where TimeGenerated >= ago(1h)
| summarize count() by bin(TimeGenerated, 5m)
| render timechart
```

### ğŸ¯ **Production Deployment Checklist**

- âœ… **Custom Tables**: Created with proper schema and retention
- âœ… **DCEs**: Deployed with appropriate network access settings  
- âœ… **DCRs**: Configured with correct stream declarations and transformations
- âœ… **ASIM DCRs**: 8 schema-specific DCRs following Microsoft best practices
- âœ… **Authentication**: App registration with Monitoring Metrics Publisher role
- âœ… **Testing**: PowerShell scripts validated with sample data
- âœ… **Monitoring**: Log Analytics queries for ingestion validation
- âœ… **Documentation**: Field mappings and data flow documented
- âœ… **Dependencies**: Terraform module ordering enforced

This comprehensive DCR implementation enables Azure Monitor to function as a scalable data ingestion platform similar to Splunk, with full Infrastructure as Code management through Terraform. The 8 ASIM schema DCRs provide dedicated performance limits and follow Microsoft's recommended architecture patterns for optimal scalability.

## ğŸ›¡ï¸ Auto-Generated Detection Wiki

The framework includes a comprehensive **Detection Wiki** system that automatically generates documentation from your detection rules:

A comprehensive system that generates Confluence-compatible documentation from Microsoft Sentinel detection rules.

### ğŸ¨ **Live Wiki Screenshots**

![Wiki Overview Dashboard](Examples/Detection_Rules_Wiki_Dashboard.png)

*Professional dashboard with statistics, MITRE ATT&CK coverage analysis, and rule distribution*

![All Detection Rules Table](Examples/All_Detection_Rules_Table.png)

*Complete rule inventory with severity, status, tactics, techniques, and GitHub links*

![Detailed Rule Information](Examples/Detailed_Rule_Information.png)

*Comprehensive rule details with MITRE mappings, source links, and technical specifications*

**Wiki Features:**
- âœ… **Auto-Generated**: Powered by PowerShell script reading YAML configurations
- âœ… **MITRE ATT&CK Coverage**: Simplified dashboard-style coverage analysis with all 14 tactics
- âœ… **Rule Statistics**: Enabled/disabled counts, severity distribution, coverage density
- âœ… **GitHub Integration**: Direct links to rule source files
- âœ… **CI/CD Integration**: Automated regeneration on rule changes via GitHub Actions
- âœ… **Confluence Integration**: Successfully deployed (Version 30) with optimized HTML formatting
- âœ… **Intelligent Metadata**: Rule-type-specific handling eliminating inappropriate UNK values
- âœ… **Kind Badges**: Visual rule categorization with emojis (ğŸ“Š Scheduled, âš¡ NRT, ğŸ›¡ï¸ Microsoft)

**Generated Documentation:**
```
DetectionWiki/
â”œâ”€â”€ DetectionWiki.md          # Main documentation (22.77 KB, 12 rules, dashboard-style MITRE coverage)
â””â”€â”€ wiki-metadata.json       # Generation metadata and statistics
```

**Key Wiki Content:**
- **MITRE ATT&CK Coverage**: Clean single-row dashboard format showing all 14 tactics horizontally
  - Rules count per tactic (e.g., InitialAccess: 2, Execution: 1)
  - Top techniques with counts (e.g., "T1078 +2 more")
- **Rule Summary Table**: Kind badges, intelligent severity, status, tactics, techniques, and GitHub links
- **Detailed Rule Information**: Individual rule documentation with rule-type-specific details
- **Coverage Statistics**: 7 of 14 tactics covered, 0.73 techniques per enabled rule
- **Auto-Generated Links**: Direct GitHub repository source links

**Automation:**
- **PowerShell Generator**: `generate-detection-wiki-clean.ps1` with intelligent metadata functions
- **GitHub Actions**: Automated wiki regeneration on content changes
- **Confluence Publishing**: Live deployment enabling analysts to quickly search analytics descriptions, use cases, and configurations.

### ğŸ“‹ Watchlist Structure

Watchlists use a **hybrid approach** combining metadata and data files for optimal management:

**File Structure:**
```
content/watchlists/
â”œâ”€â”€ known-bad-ips-metadata.yaml    # Watchlist metadata (name, description, search_key)
â”œâ”€â”€ known-bad-ips.csv              # Tabular data (IP addresses, threat types, etc.)
â”œâ”€â”€ suspicious-domains-metadata.yaml
â”œâ”€â”€ suspicious-domains.csv
â”œâ”€â”€ vip-users-metadata.yaml
â””â”€â”€ vip-users.csv
```

**Benefits of CSV Format:**
- âœ… **Industry Standard**: Native format for threat intelligence feeds
- âœ… **Easy Management**: Import/export from Excel, databases, threat feeds
- âœ… **Simple Structure**: Column-based format matches Sentinel's expectations
- âœ… **Analyst Friendly**: Security teams can easily maintain IOC lists
- âœ… **Terraform Native**: Uses built-in `csvdecode()` function for parsing

**Example CSV Structure:**
```csv
IPAddress,ThreatType,Confidence,FirstSeen,Source
192.0.2.1,Malware,High,2025-01-15T10:30:00Z,Internal Analysis
203.0.113.5,Phishing,High,2024-02-10,External Feed
```

## ğŸ”’ Security Considerations

- All sensitive variables stored in Terraform Cloud workspace variables
- State file managed remotely in Terraform Cloud
- Access controlled through Azure RBAC and Terraform Cloud permissions

## ğŸ“ˆ Environment Strategy

The framework uses a **simplified single-configuration approach**. For multiple environments, consider:

**Workspace Configuration** (Recommended)
- Use Terraform Cloud workspaces with environment-specific variable sets
- Single repository, multiple workspaces approach

This prevents accidental deployments and allows environment-specific customizations.

## ğŸ“š Documentation

**Key Documentation Resources:**
- ğŸ“– **Getting Started Guide**: Complete setup instructions above
- ğŸ—ï¸ **Architecture**: Individual modules in `modules/` directories with comprehensive documentation
- ğŸ“‹ **Content Management**: Individual YAML files in `content/` directories for granular control
- ğŸ“š **Auto-Generated Wiki**: Comprehensive Detection Wiki with MITRE ATT&CK mapping
- ğŸ”— **Data Collection**: DCRs & DCEs documentation in sections above
- ğŸ“Š **Remote State Management**: Cross-workspace Terraform Cloud integration

**External References:**
- [Terraform AzureRM Provider Documentation](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs)
- [Azure Sentinel GitHub](https://github.com/Azure/Azure-Sentinel)
- [Microsoft ASIM Documentation](https://docs.microsoft.com/en-us/azure/sentinel/normalization)


**Last Updated**: August 5, 2025  
